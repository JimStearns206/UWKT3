---
title: "Kaggle West Nile Virus Competition"
author: "Team UWKT3"
date: "9 June 2015"
output:
    pdf_document:
        toc: true
geometry: margin=0.25in
fontsize: 12pt
mainfont: Verdana
monofont: Consolas
---
# 0. Introduction

The culminating project of the Spring 2015 UW PCE Data Science course "Data at Scale" was to participate in a Kaggle data science competition. Instructor Dr. Barga chose the [West Nile Virus Prediction](https://www.kaggle.com/c/predict-west-nile-virus) competition.

The class was broken up into three teams of roughly 8 students. This is the report of the third team with the Kaggle name of UWKT3.

## 0.1 Background

The West Nile Virus Prediction (WNVP) competition's goal was to "Predict West Nile virus in mosquitos across the city of Chicago.":

![KaggleWnvpHomePage](screenshots/KaggleWnvpHomePage.png)

The WNVP contest started on April 22nd and will end 17 June 2015. UWKT3's first submission was on May 12th. Its last so far was on June 4th.

## 0.2 Team Goals

1. Experiment with modelling alternative on a real-world dataset.
2. Learn how to participate in a Kaggle competition.
3. Non-Goal: win the competition, or even score highly.

## 0.3 Team Members

* Bethene Britt
* Andrew Ewing
* Gregory Hogue
* Patrick Leahy
* Linghua Qiu
* Chris Ross
* Robert Russell
* Jim Stearns

# 1. Data Preparation

A goal of the team project was to create a "golden" train and test dataset that could form the basis of many modeling experiments.

## 1.1 Obtain Original Datasets from Kaggle Website

Download the training, test, spray, and weather data from the Kaggle web site page for West Nile Virus Prediction.

Please see Appendix for R (and Python) code that:

* General Setup: Clears environment, sets working directory, loads libraries, define utility functions
* Downloads the data from the Kaggle site and unzips them. 

```{r Setup, echo=FALSE, results='hide',message=FALSE}
# Clear the working environment of variables, data, functions
rm(list=ls())

# Set working directory for this Kaggle project. Default: pwd.
#kaggleProjHomeDir <- "."
kaggleProjHomeDir <- "/Users/jimstearns/GoogleDrive/Learning/Courses/UWPCE-DataScience/Course3_DataAtScale/KaggleProject/Reports"
setwd(kaggleProjHomeDir)
getwd()

#install.packages("rPython")  # For download from web site with login/pwd.
library(rPython) # For calling python function to download file w/login+pwd
# Package for writing Weka ARFF file format
stopifnot(require("foreign"))
library("foreign")
# Package for calculating great circle distances
stopifnot(require("geosphere"))
library("geosphere")

# Return a data frame with the named column(s) moved to last position. 
# Intended usage: move the output classification, WnvPresent, to the last column position.
moveColsToLast <- function(df, colsToMove) {
    df[c(setdiff(names(df), colsToMove), colsToMove)]
}

moveColsToFirst <- function(df, colsToMove) {
    df[c(colsToMove, setdiff(names(df), colsToMove))]
}
```

```{r GetDatasetsIntoInputDir, echo=FALSE, results='hide',message=FALSE}
wnvpTrainFilename <- "train.csv"
wnvpTestFilename <- "test.csv"
wnvpWeatherFilename <- "weather.csv"
wnvpSprayFilename <- "spray.csv"
kaggleDatasets = c(
    wnvpTrainFilename, 
    wnvpTestFilename, 
    wnvpWeatherFilename, 
    wnvpSprayFilename)
dataSubDir <- "input"  # Kaggle convention
workingSubDir <- "working" # Kaggle convention: massaged datasets - and output - go here.

wnvpTrainFileNRecs <- 10506 # Observation records in training file. Excludes header record.
wnvpTestFileNRecs <- 116293 # Records in test file supplied by Kaggle. Submission record cnt must match.
# If download from Kaggle required, and user and pwd are empty (default),
# then user will be prompted for these two values.
kaggleUsername <- ""
kagglePassword <- ""

allKaggleFilesArePresent <- function() {
    filesAllFound <- TRUE
    for (file in kaggleDatasets) {
        if (!file.exists(paste0(dataSubDir, "/", file))) {
            print(paste("Error: could not find unzipped Kaggle file in PWD:", file))
            filesAllFound <- FALSE
        }
    }
    return(filesAllFound)
}

downloadMissingKaggleFiles <- function() {
    python.load("src/UrlFileDownloaderWithLogin.py")
    
    kaggleUsername = Sys.getenv("kaggleUsername")
    kagglePassword = Sys.getenv("kagglePassword")
    if (kaggleUsername == "" || kagglePassword == "") {
        print("Please assign kaggleUsername and kagglePassword environment variables.")
        print("Place in ~/.Renviron entries such as kaggleUsername='YourName'.")
    }
    stopifnot(!(kaggleUsername == ""))
    stopifnot(!(kagglePassword == ""))
    
    wnvpKaggleDataUrl <- 
        "https://www.kaggle.com/c/predict-west-nile-virus/download/"
    
    for (file in kaggleDatasets) {
        if (file.exists(file))
            next
        
        urlOfZip <- paste0(wnvpKaggleDataUrl, file, ".zip")
        print(urlOfZip)
        # Use a python method to download from URL with login and password.
        # Download to subdirectory "input" and filename w/o the .zip suffix.
        python.call("Download", urlOfZip, 
                    kaggleUsername, kagglePassword , 
                    paste0(dataSubDir, "/", file, ".zip"))
    }
}

unzipDownloadedFiles <- function() {
    for (file in kaggleDatasets) {
        zippedFile <- paste0(dataSubDir, "/", file, ".zip")
        print(paste0("Unzip: ", zippedFile))
        if (file.exists(zippedFile)) {
            if (file.exists(file)) {
                print(sprintf("Warning: removing existing file %s\n", file))
                file.remove(file)
            }
            unzip(zippedFile, exdir=dataSubDir)
            print(sprintf("Unzipped: %s\n", zippedFile))
        }
    }
}

if (!allKaggleFilesArePresent()) {
    print(paste("Not all needed Kaggle datasets are present in PWD;",
                "attempting to download from Kaggle web site."))
    downloadMissingKaggleFiles()
    unzipDownloadedFiles()
}

```

```{r}
stopifnot(allKaggleFilesArePresent())
print("All unzipped Kaggle datasets found in PWD. Proceeding.")
```

## 1.2 Read Kaggle Train/Test Files

* Read in test and train csv-format files into data frames.

```{r CreateMasterTrainAndTestDatasets}
test_df <- read.csv(paste0(dataSubDir, "/", wnvpTestFilename))
train_df <- read.csv(paste0(dataSubDir, "/", wnvpTrainFilename))
# Quick sanity check: got right number of records?
stopifnot(nrow(train_df) == wnvpTrainFileNRecs)
stopifnot(nrow(test_df) == wnvpTestFileNRecs)
```

## 1.3 Feature Selection/Creation

* Make the train and test datasets have the same attributes:
    * Train: Convert the WnvPresent column from numeric to factor with levels "Yes" and "No".
    * Train: Add an Id attribute, set to zero.
    * Train: Remove NumMosquitos attribute. Potentially useful, but not available in Test dset.
    * Test: Add a WnvPresent factor column, all with "No" level.
    * Both: Remove the address attributes of little use compared to Lat/Long: 
        * Address, Block, Street, AddressNumberAndStreet, AddressAccuracy.
    * Both: Add bit vectors of each of the levels of the Species factor (a new column for each of the factor levels, with a zero or 1 value). Leave the Species as well.
    * Both: Convert date into date format, add "Year", "Month", and "Week" factor attributes.
   
```{r FeatureFixupAndCreation}

# WnvPresent. Train: convert to factor. Test: add as factor, default value of "No".
train_df$WnvPresent <- factor(train_df$WnvPresent, labels=c("No", "Yes"))
WnvPresent <- factor("No", levels=c("No","Yes"))
test_df <- cbind(test_df, WnvPresent)

# Train: Add Id attribute to match that in Test. Set to 0. Id in Test is 1-relative.
train_df["Id"] <- 0
train_df <- moveColsToFirst(train_df, "Id")

# Train: Remove NumMosquitos attribute. Potentially useful, but not available in Test dset.
train_df$NumMosquitos <- NULL

# Both: Remove the block attributes of little use: 
attrsToRemove <- c("Address", "Block", "Street", "AddressNumberAndStreet", "AddressAccuracy")
train_df <- train_df[,!names(train_df) %in% attrsToRemove]
test_df <- test_df[,!names(test_df) %in% attrsToRemove]

# For creation of factor attributes, temporarily combine train and test into one dataset
# so that factor levels are the same when both are written out as separate files.
# Keeps Weka happy.
# Add a temporary column distinquishing train from test dataset entries.
train_df$DsetType <- "Train"
test_df$DsetType <- "Test"

combined_df = rbind(train_df, test_df)

# Both (in Combined): Add bit vectors for Species, one column for each factor level
# TODO: "UNSPECIFIED CULEX" needs attention.
combined_df <- with(combined_df, cbind(model.matrix( ~ 0 + Species, combined_df), combined_df))

# Both (in Combined): Convert date into date format, 
# add "Year", "Month", and "Week" factor attributes.
# as.Date() tries %Y-%m-%d by default, but what the heck, explicitly state the format.
combined_df$Date <- as.Date(combined_df$Date, format="%Y-%m-%d")

combined_df$Year <- as.factor(format(combined_df$Date, "%Y"))
combined_df$Month <- as.factor(format(combined_df$Date, "%m"))
combined_df$Week <- as.factor(format(combined_df$Date, "%U"))

# Move temporary dsetType and date-related attributes to left, leaving WnvPresent last
combined_df <- moveColsToFirst(combined_df, c("DsetType", "Id", "Date", "Year", "Month", "Week"))

# Do not remove the Species attribute - not all models will use the bit vectors.
#train$Species <- NULL
#test$Species <- NULL
```

## 1.4 Feature Creation (ctd): Merge Weather Data with Trap Observations in Train/Test Datasets

### Calculate Distance of Trap from the Two Weather Stations

* Both (in Combined): Calculate the distance (using lat/long) of the trap from the two weather stations, adding attributes with the value in kilometers. Patience: This takes a while (~5 minutes).
* Both (in Combined): Add a nearest weather station attribute.

Using function *distCosine* in [R Geosphere Package](http://cran.r-project.org/web/packages/geosphere/geosphere.pdf) to calculate distance on a sphere.

```{r WeatherStationDistances, cache=TRUE} 
# Station 1: O'Hare
station1LongLat <- c(-87.933, 41.995)
    
# Station 2: Midway
station2LongLat <- c(-87.752, 41.786)

# Patience. This takes a while (~5 minutes)
for (i in 1:nrow(combined_df)) {
    combined_df$Station1DistKm[i] <- distCosine(
        c(combined_df$Longitude[i], combined_df$Latitude[i]), station1LongLat) / 1000
    combined_df$Station2DistKm[i] <- distCosine(
        c(combined_df$Longitude[i], combined_df$Latitude[i]), station2LongLat) / 1000
}
combined_df$NearestStation <- ifelse(
    combined_df$Station1DistKm <= combined_df$Station2DistKm, 1, 2)
```

* Both (in Combined): Merge in temperature and wind data from nearest station on that date.

```{r MergeWeatherData}
weather_df <- read.csv(paste0(dataSubDir, "/", wnvpWeatherFilename), stringsAsFactors=FALSE)
colsToKeep <- c("Station", "Date", "Tmax", "Tmin", "Tavg", "AvgSpeed")
weatherData <- weather_df[,names(weather_df) %in% colsToKeep]
weatherData$Date <- as.Date(weatherData$Date, format="%Y-%m-%d")
# Tmax and Tmin come in as type int. Tavg, however, comes in as chr.
weatherData$Tavg <- as.integer(weatherData$Tavg)
# So does AvgSpeed.
weatherData$AvgSpeed <- as.numeric(weatherData$AvgSpeed)

combinedww <- merge(combined_df, weatherData, 
                             by.x=c("Date", "NearestStation"), by.y=c("Date", "Station"), 
                             all.x=TRUE)
#str(combinedww)

# Make "Id" the first column and "WnvPresent" the last.
combinedww <- moveColsToFirst(combinedww, "Id")
combinedww <- moveColsToLast(combinedww, "WnvPresent")

stopifnot(nrow(combinedww) == (wnvpTrainFileNRecs + wnvpTestFileNRecs))
```

Weather data does have some NA fields (warning above: "Warning: NAs introduced by coercion"), 
but not the subset merged into train and test dset.
Throw an exception if that ever proves not to be the case.

```{r StopIfWeatherDataUsedContainsNA}
stopifnot(sum(is.na(combinedww$Tmin)) == 0)
stopifnot(sum(is.na(combinedww$Tmax)) == 0)
stopifnot(sum(is.na(combinedww$Tavg)) == 0)
stopifnot(sum(is.na(combinedww$AvgSpeed)) == 0)
```

## 1.5 Write "master" train and test files, complete (all attributes), to CSV and ARFF

* Write the train and test datasets - including weather data - as CSV.

```{r WriteTrainAndTestMasterDatasetsAsCsv}
stopifnot(sum(combinedww$DsetType == "Train") == wnvpTrainFileNRecs)
stopifnot(sum(combinedww$DsetType == "Test") == wnvpTestFileNRecs)

trainWithWeather <- combinedww[combinedww$DsetType == "Train",]
stopifnot(nrow(trainWithWeather) == wnvpTrainFileNRecs)
trainWithWeather$DsetType <- NULL

write.csv(trainWithWeather, 
           paste0(workingSubDir, "/", "train", "Master", ".csv"), 
           eol = '\n')
str(trainWithWeather)

testWithWeather <- combinedww[combinedww$DsetType == "Test",]
stopifnot(nrow(testWithWeather) == wnvpTestFileNRecs)
# Make sure test dataset is still ordered by Id
testWithWeather <- testWithWeather[order(testWithWeather$Id),]
trainWithWeather$DsetType <- NULL

write.csv(testWithWeather, 
           paste0(workingSubDir, "/", "test", "Master", ".csv"), 
           eol = '\n')
#str(testWithWeather)
```

* Write ARFF versions as well. Advantage over CSV: levels of factors are maintained,
even if no observations have that level. CSV builds levels from usage.

```{r WriteTrainAndTestMasterDatasetsAsArff}
write.arff(trainWithWeather, 
           paste0(workingSubDir, "/", "train", "Master", ".arff"), 
           eol = '\n')

write.arff(testWithWeather, 
           paste0(workingSubDir, "/", "test", "Master", ".arff"), 
           eol = '\n')
```

# 2. Data Exploration and Analysis

*TBD*

# 3. Features for Modeling

*TBD*

# 4. Kaggle Submissions

![UWKT3 Kaggle Submissions](screenshots/UWKT3_WnvpSubmittals.png)

## 4.1 May 17 Pat Leahy Submittal (Decision Tree, Score 0.59642)

Pat's summary of data preparation, feature selection, and model:

### Data Preparation

We joined the weather data provided by Kaggle to the training and test records. 
This resulted in two new tables which contained the test and training data 
along with a set of weather attributes from the nearest weather station for the 
date in question.

We carried out our data preparation in Excel. We copied the files train.csv, 
test.csv and weather.csv into tabs in an Excel workbook. There were two weather 
stations in the weather data. We calculated the distance from each observation 
point to each of the two weather stations. We used an Excel macro copied from 
http://www.codecodex.com/wiki/Calculate_distance_between_two_points_on_a_globe#E
xcel to calculate the distances given latitude and longitude. We then 
determined which weather station was closer to each point. We used the weather 
station ID and date as a key to join test and training records to the weather 
records. We used Excels VLOOKUP function to implement a join.

### Feature Selection

Once we had the training and test data joined to the weather data we selected 
some features to generate a model. A team member studied mosquitos and reported 
the following

"Culex mosquitoes lay their eggs usually at night on the surface of fresh or 
stagnant water; usually lay their eggs at night; a mosquito may lay a raft of 
eggs every third night during its life span.

Culex usually live only a few weeks during the warm summer months; those 
females which emerge in late summer search for sheltered areas where they 
hibernate (diapause) until spring; warm weather brings them out in search of 
water on which to lay their eggs. "

Given this knowledge we selected the Month as a feature.

Chicago has one large body of water with a coastline which runs in 
approximately a straight line. We therefore concluded that Latitude and 
Longitude would also be useful features.

We also selected three temperature measures from the weather data. They were 
Minimum Temperature, Maximum Temperature and Average Temperature. We selected 
these temperature features because they didn't contain any missing values.

The full set of features we selected were Latitude, Longitude, Month, Minimum 
Temperature, Maximum Temperature and Average Temperature.

### Model

We decided to over sample the test data to include the same number of positive 
observations for West Nile Virus as negative observations. We did this by 
selecting all the positive observations together with an equal number of 
negative observations randomly selected. We carried out the random selection in 
Excel by adding a new column of randomly generated values using the RAND 
function and then sorting using that column. We created two new CSV files, a 
training and test file, containing only our selected features. The training set 
only contained the equally represented subset of positive and negative 
observations.

We opened the training set in Weka and generated a Decision Tree using the J48 
classifier. We tuned some of the parameters until we settled on the following 
settings, "-C 0.5 -M 2". We had to reformat the class column in the training 
file to be Yes/No instead for 1/0 for Weka to recognize it as a class.
We then used the test.csv we created with only our specific features. We had 
some difficulty using the test file until we added a class column. This we just 
set to No for all records.

Weka failed to run the model if we tried to output the results of the test to a 
file regardless of the file type. To work around this we turned off output to a 
file. Instead we right clicked on the results in the result list and selected 
"Visualize classification errors". We could then save the predictions in the 
window which opened as an ARFF file. We converted this to a CSV, changed some 
of the columns and this gave me a submission file to upload to Kaggle.
We uploaded this submission and achieved an accuracy of 0.59642. This is better 
than the accuracy of 0.5 we achieved when prediction no West Nile Virus for 
every test record.

### Reproduce ARFF datasets for Use as Model Input in Weka

Read in the train and test golden "Master" datasets. Use the ARFF versions so that the factor types are preserved with the same levels, even if some levels are not present in any record
in the file.

```{r 0517PLReadInMaster}
trainRecs <- read.arff(paste0(workingSubDir, "/", "train", "Master", ".arff"))
testRecs <- read.arff(paste0(workingSubDir, "/", "test", "Master", ".arff"))
stopifnot(nrow(trainRecs) == wnvpTrainFileNRecs)
stopifnot(nrow(testRecs) == wnvpTestFileNRecs)
```

Perform any subsetting here so that train and test formats look the same to Weka.

```{r PareDownColumnsToThoseUsedIn0517PLModeling}
colsToKeep=c("Latitude", "Longitude", "Month", "Tmin", "Tmax", "Tavg", "WnvPresent")
trainRecs <- trainRecs [,names(trainRecs) %in% colsToKeep] 
testRecs <- testRecs [,names(testRecs) %in% colsToKeep] 
```

Undersample: use all the WnvPresent==True samples. Randomly select an equal number of False samples. Use that for the test data set.

```{r UndersampleWnvPresent0517PL}
curModelIdx <- "0517PL"
undersample_df <- trainRecs[trainRecs$WnvPresent=="Yes",]
nFalseObservationsToUse <- nrow(undersample_df)
wnvNotPresent <- trainRecs[trainRecs$WnvPresent=="No",]
undersample_df <- rbind(undersample_df,
                       wnvNotPresent[sample(nrow(wnvNotPresent), nFalseObservationsToUse),])

write.arff(undersample_df, 
           paste0(workingSubDir, "/", "train", curModelIdx, ".arff"), 
           eol = '\n', relation="WNVPTrainDataset")
str(undersample_df)

write.arff(testRecs, 
           paste0(workingSubDir, "/", "test", curModelIdx, ".arff"), 
           eol = '\n', relation="WNVPTestDataset")
str(testRecs)
```

### Screenshot of Leaderboard

![May 17 Pat Leahy Submittal](../Submissions/Submission_0517_PL/Screenshot_UWKT3_Leaderboard_Submission20150517_BB.png)

## 4.2 May 28 Jim Stearns Submittal (Decision Tree, Score 0.62835)

Based upon May 17 Pat Leahy Submittal (Score 0.59642), with one additional predictor: Species bit vectors.

### Feature Selection

Input: 

* Attributes: Lat/Long, Month, Tmin/Tmax/Tavg, Species bit vectors.
* Classified Attribute: WnvPresent. Two-level factor: "No", "Yes".
* Observations: all WnvPresent records, plus an equal number of !WnvPresent records, randomly sampled.
        
Output: train and test datasets in ARFF format for modeling in Weka.

```{r 0528JSReadInMaster}
trainRecs <- read.arff(paste0(workingSubDir, "/", "train", "Master", ".arff"))
testRecs <- read.arff(paste0(workingSubDir, "/", "test", "Master", ".arff"))
stopifnot(nrow(trainRecs) == wnvpTrainFileNRecs)
stopifnot(nrow(testRecs) == wnvpTestFileNRecs)
```

```{r PareDownColumnsToThoseUsedInArffModeling0528JS}
colsToKeep=c("Latitude", "Longitude", "Month", "Tmin", "Tmax", "Tavg", "WnvPresent")
colsToKeep=c(colsToKeep, "SpeciesCULEX ERRATICUS", "SpeciesCULEX PIPIENS", 
             "SpeciesCULEX PIPIENS/RESTUANS", "SpeciesCULEX RESTUANS", "SpeciesCULEX SALINARIUS",
             "SpeciesCULEX TARSALIS", "SpeciesCULEX TERRITANS", "SpeciesUNSPECIFIED CULEX")
trainRecs <- trainRecs [,names(trainRecs) %in% colsToKeep] 
testRecs <- testRecs [,names(testRecs) %in% colsToKeep] 
```

Undersample: use all the WnvPresent==True samples. Randomly select an equal number of False samples. Use that for the training data set.

```{r UnderSampleNotWnvPresent0528JS}
curModelIdx <- "0528JS"
allWnvPresentTrainRecs <- trainRecs[trainRecs$WnvPresent=="Yes",]
nFalseObservationsToUse <- nrow(allWnvPresentTrainRecs)
allNotWnvPresentTrainRecs <- trainRecs[trainRecs$WnvPresent=="No",]
sampleNotWnvPresentTrainRecs <- allNotWnvPresentTrainRecs[
    sample(nrow(allNotWnvPresentTrainRecs), nFalseObservationsToUse),]

train_0528JS <- rbind(allWnvPresentTrainRecs, sampleNotWnvPresentTrainRecs)
write.arff(train_0528JS, 
           paste0(workingSubDir, "/", "train", curModelIdx, ".arff"), 
           eol = '\n', relation="WNVPTrainDataset")
str(train_0528JS)

# Write all the test recs
write.arff(testRecs, 
           paste0(workingSubDir, "/", "test", curModelIdx, ".arff"), 
           eol = '\n', relation="WNVPTestDataset")
#str(testRecs)
```

### Model

Weka Modeling: same as Pat Leahy using on May 17th:

* Opened the training set in Weka. Had
* Generated a Decision Tree using the J48 classifier. 
* Used Pat's parameter settings, "-C 0.5 -M 2".

### Notes

* NumMosquitos degraded score. It's not an attribute in test dataset. Not using.
* Dataset ideosynchrocy not dealt with: Dataset rolls over to a new record if number of mosquitos reaches 50.
    * TODO: Combine records with same date, same lat/long, same Species. Sum NumMosquitos, set WnvPresent if any record is WnvPresent.
    
### Screenshot of Leaderboard

![May 28 Jim Stearns Submittal](../Submissions/Submission_0528_JS_2/Submission_0528_JS_2_Leaderboard.png)

## 4.3 June 2 Andy Ewing Submittal (Generalized Additive Model (GAM), Score 0.71862)

Submitted by Andy Ewing. Used a sample from the Kaggle WNVP forum: [baby steps: breach 0.71 with GAM](https://www.kaggle.com/c/predict-west-nile-virus/forums/t/13754/baby-steps-breach-0-71-with-gam/75935)

Added weather data: daily average temperature and wind speed.

Modeling is done in R, not Weka.

```{r sourceGam, cache=TRUE}
source("src/starter_GAM.R", echo=TRUE, verbose=FALSE, print.eval=FALSE, 
       prompt.echo=" ", continue.echo="  ")
```

### Screenshot of Leaderboard

![June 2 Andy Ewing Submittal](../Submissions/Submission_0602_AE_71862/ewingSubmission2_Leaderboard.jpg)

# 5. Modeling Strategy

*TBD. Unclear: discuss modeling strategy here, or in discussion of submittals?*

# 6. Ensemble Model Opportunities

# Appendix

## A.1 General Setup: Clear environment, set working directory, load libraries, utilities

```{r Setup,eval=FALSE}
```

## A.2 Dataset download and unpacking

This R and Python code downloads the WNVP datasets from Kaggle. 

Why use Python instead of R? Why not just use read.csv("https://www.gaggle.com/datalocation")?

The read.csv() function does not support SSL when reading from a URL.

R's download.file(<https url>, <destfile>, method="curl") does download HTTPS URLs, but has no facility for establishing authentication credentials for a session. I.e. Kaggle requires a Kaggle login in order to download data files.

There may be a way in R, but I found a way in Python, and using it from R using RPython works.

Some setup is required:

* One's Kaggle username and password must be
defined as environment variables where R is running.
* Easiest way to set environment variable for R: Create (add to) ~/.Renviron file (kaggleUsername="XXXX" and kagglePassword="YYYY").

```{r GetDatasetsIntoInputDir,eval=FALSE}
``` 

Alternatively, files can be downloaded manually.
  
### File UrlFileDownloaderWithLogin.py:
```
__author__ = 'jimstearns'
""" Download a file at a URL at a web site that requires a user name and password.
"""

import logging
import os       # File utilities

# Python package "requests": "Python HTTP for Humans" by Kenneth Reitz. Current version: 2.7.0.
# Documented at http://docs.python-requests.org/en/latest/
# To install from the command line: "pip install requests"
# (On Mac, sudo may be required. Also pip2.7 instead of pip, depending on default Python version)

import requests # Http GET, POST

def Download(url, username, password, local_filename):
    # Login to web site such as Kaggle and retrieve the data. Use POST rather than GET as as to
    # send login info in body of HTTP request rather than in query string portion of URL.
    
    # Limitation: when used by Python version < 2.7.9, an "InsecureRequestWarning" is generated.
    # TODO: Fix. Details: https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning
    # Workaround: log warnings to file, not stdout.
    logging.captureWarnings(True)

    if (os.path.exists(local_filename)):
        os.remove(local_filename)

    # This won't get the file, but use the return value URL in a follow-on POST:
    r = requests.get(url)

    login_info = {'UserName': '{0}'.format(username), 'Password': '{0}'.format(password) }
    print(login_info)
    r = requests.post(r.url, data = login_info)
    print("POST (w/login info): {0}\n".format(r.status_code))

    # Write the data to a local file one chunk at a time.
    chunk_size = 512 * 1024 # Reads 512KB at a time into memory
    with open(local_filename, 'wb') as fd:
        for chunk in r.iter_content(chunk_size): # Reads 512KB at a time into memory
            if chunk: # filter out keep-alive new chunks
                fd.write(chunk)

    if (os.path.exists(local_filename)):
        return(True)
    else:
        return(False)
```

## A.3 Prepare Weka results as ARFF file as submittal file to Kaggle as CSV

File PrepareWekaArffResultsForKaggleCsvSubmittal: 

```
# Script to read in ARFF file created by Weka modeler,
# strip all attributes except the predicted classification (here, "WnvPresent"),
# add an Id column with a sequence number equal to the record number; and
# write as a CSV file.

library("foreign") # For read.arff
wnvpTestFileNRecs <- 116293 # Records in test file supplied by Kaggle. Submission record cnt must match.

dataSubDir <- "../Submissions/Submission_0604_JS_1/" # Modify as needed
fileBaseName <- "test0528JS_WekaClassified"  # Change for your filename. Note: no suffix.

fileBasePath <- paste0(dataSubDir, fileBaseName)
testClassified_df <- read.arff(paste0(fileBasePath, ".arff"))
stopifnot(nrow(testClassified_df) == wnvpTestFileNRecs)

Id <- seq(1:wnvpTestFileNRecs)
colsToKeep <- c("predicted WnvPresent")
testClassified_df <- cbind(Id, testClassified_df[names(testClassified_df) %in% colsToKeep])
names(testClassified_df) <- c("Id", "WnvPresent")
# Write "No" as 0 and "Yes" as 1
testClassified_df$WnvPresent <- ifelse(testClassified_df$WnvPresent == "No", 0, 1)
str(testClassified_df)

write.csv(testClassified_df, paste0(fileBasePath, ".csv"), row.names=FALSE)
```